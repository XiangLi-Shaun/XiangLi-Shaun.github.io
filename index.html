<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Xiang "Shaun" Li, Assistant Professor, Massachusetts General Hospital and Harvard Medical School</title>

  <!-- Import two fonts:
       1) Montserrat Black for large headings
       2) Source Code Pro for body text (monospace) -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link 
    href="https://fonts.googleapis.com/css2?family=Montserrat:wght@900&family=Source+Code+Pro:wght@400;700&display=swap" 
    rel="stylesheet"
  >

  <style>
    /* -----------------------------------------------------
       BASE STYLES & RESET
    ------------------------------------------------------ */
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    html, body {
      height: 100%;
    }
    body {
      /* White background */
      background: #ffffff;
      /* Use monospace for body text */
      font-family: 'Source Code Pro', monospace;
      /* Black-themed body text */
      color: #111111;
      line-height: 1.6;
      padding: 20px;
    }

    /* CSS Variables for easier theming */
    :root {
      --accent-color: #8ae234;         /* Terminal-like green */
      --accent-color-light: #b3f75e;   /* Lighter hover shade */
      --border-color: #dddddd;         /* Light border lines */
      --bg-card: #fafafa;             /* Light card background */
      --heading-color: #000000;       /* Headings in deep black */
    }

    /* -----------------------------------------------------
       LINKS & GENERAL ELEMENTS
    ------------------------------------------------------ */
    a {
      color: var(--accent-color);
      text-decoration: none;
      transition: color 0.2s ease-in-out;
    }
    a:hover {
      color: var(--accent-color-light);
      text-decoration: underline;
    }

    hr {
      border: none;
      border-top: 1px solid var(--border-color);
      margin: 30px 0;
    }

    /* -----------------------------------------------------
       WRAPPER / CONTAINERS
    ------------------------------------------------------ */
    .container {
      max-width: 900px;
      margin: 0 auto;
    }

    /* -----------------------------------------------------
       HEADER
    ------------------------------------------------------ */
    header {
      display: flex;
      align-items: center;
      justify-content: space-between;
      padding-bottom: 10px;
      margin-bottom: 20px;
      border-bottom: 2px solid var(--border-color);
    }
    /* Heaviest weight heading font */
    header h1 {
      font-size: 2rem;
      font-family: 'Montserrat', sans-serif;
      font-weight: 900;  /* black-style font weight */
      color: var(--heading-color);
      letter-spacing: -1px; /* optional slight tightening */
    }
    nav a {
      margin-left: 20px;
      font-weight: 700;
    }

    /* -----------------------------------------------------
       SECTION TITLES
    ------------------------------------------------------ */
    h2 {
      font-family: 'Montserrat', sans-serif;
      font-weight: 900;
      font-size: 1.5rem;
      color: var(--heading-color);
      margin-bottom: 15px;
      border-bottom: 2px dashed var(--accent-color);
      display: inline-block;
      padding-bottom: 5px;
    }

    /* -----------------------------------------------------
       ABOUT SECTION
    ------------------------------------------------------ */
    #about p {
      margin-bottom: 20px;
      font-size: 1rem;
    }

    /* -----------------------------------------------------
       UPDATES / BLOG-LIKE SECTION
    ------------------------------------------------------ */
    .updates {
      margin-bottom: 40px;
    }
    .update {
      margin-bottom: 20px;
      padding: 15px;
      background: var(--bg-card);
      border-left: 5px solid var(--accent-color);
      border-radius: 4px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.08);
      transition: transform 0.2s ease;
	  
    }
    .update:hover {
      transform: translateY(-2px);
    }
    .update-date {
      font-size: 0.9rem;
      color: #555555;
      margin-bottom: 8px;
    }
    .update p {
      font-size: 1rem;
      color: #222222;
    }

    /* -----------------------------------------------------
       PUBLICATIONS
    ------------------------------------------------------ */
    .publications .publication {
      margin-bottom: 15px;
      padding: 15px;
      background: var(--bg-card);
      border-left: 5px solid var(--accent-color);
      border-radius: 4px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.08);
      transition: transform 0.2s ease;
    }
    .publications .publication:hover {
      transform: translateY(-2px);
    }
    .publications strong {
      color: var(--accent-color);
      font-weight: 700;
    }
    .publications a {
      font-size: 0.95rem;
    }

    /* -----------------------------------------------------
       FOOTER
    ------------------------------------------------------ */
    footer {
      margin-top: 50px;
      padding-top: 20px;
      border-top: 2px solid var(--border-color);
      text-align: center;
      font-size: 0.9rem;
      color: #666666;
    }
    footer p {
      margin-bottom: 10px;
    }
    footer a {
      margin: 0 5px;
    }

	.collapsible.collapsed {
  max-height: 800px; /* Adjust this height to show a few updates initially */
  overflow: hidden;
  position: relative;
  transition: max-height 0.4s ease-in-out;
  /*overflow-y: scroll;*/      /* Always show vertical scrollbar */
}

.collapsible.expanded {
  max-height: 999999px;
  
}

#toggle-updates {
  display: inline-block;
  margin-top: 10px;
  font-weight: bold;
  color: var(--accent-color, #8ae234);
  cursor: pointer;
  font-family: 'Source Code Pro', monospace;
}

.update::-webkit-scrollbar {
  width: 8px;
}

.update::-webkit-scrollbar-thumb {
  background: #ccc;
  border-radius: 4px;
}

.update::-webkit-scrollbar-track {
  background: transparent;
}

.about-container {
  display: flex;
  flex-wrap: wrap;
  gap: 40px;
}

.about-bio {
  flex: 1 1 40%;
}

.about-contact {
  flex: 1 1 35%;
  background: #f9f9f9;
  padding: 20px;
  border-left: 4px solid #111111;
  border-radius: 6px;
  box-shadow: 0 2px 6px rgba(0, 0, 0, 0.05);
}

.about-contact h3 {
  margin-bottom: 10px;
}

.about-contact p {
  margin-bottom: 10px;
  font-size: 0.95rem;
}

.about-links {
  list-style-type: none;
  padding-left: 0;
  font-size: 0.95rem;
}

.about-links li {
  margin-bottom: 8px;
}

.about-list {
  padding-left: 1.2rem;
  margin: 15px 0;
}

.about-list li {
  margin-bottom: 10px;
}
  </style>
  
</head>

<body>

  <div class="container">
    <!-- HEADER -->
    <header>
      <h1>Xiang Li</h1>
      <nav>
        <a href="#about">About</a>
        <a href="#updates">Updates</a>
        <a href="#publications">Publications</a>
        <a href="#contact">Contact</a>
      </nav>
    </header>

    <!-- ABOUT SECTION -->
	<section id="about" class="about-section">
		<div class="about-container">
		  <!-- Left Column: Bio and Services -->
		  <div class="about-bio">
			<p>
			  I am an <strong>Assistant Professor</strong> at the <strong>Massachusetts General Hospital (MGH)</strong> and <strong>Harvard Medical School (HMS)</strong>, Department of Radiology, and an Affiliate Faculty Member at the <strong>Kempner Institute for Natural and Artificial Intelligence of Harvard University</strong>.
			</p>
			<p>
			  I received my postdoc training at MGH/HMS under the co-mentorship of Associate Prof. <strong>Quanzheng Li</strong> and Distinguished Professor, National Academy of Medicine member <strong>James H. Thrall</strong>. I earned my Ph.D. in Computer Science from the <strong>University of Georgia</strong>, supervised by AIMBE fellow, Distinguish Prof. <strong>Tianming Liu</strong>.
			</p>
			<p>
			  My primary research interest is artificial intellegnece for healthcare, with a focus on <strong>medical foundation models, <strong></strong>medical generative AI</strong>, <strong>medical informatics</strong>, <strong>counterfactual causal inference</strong>, and <strong>neuroimaging</strong>. 
			</p>
      <p>
        My <a target="_blank" href="./pdf/CV_xiang.pdf">Curriculum Vitae (PDF)</a>.
      </p>
      <p>Visit my <a target="_blank" href="https://www.linkedin.com/in/xiang-li-11b2b99/">LinkedIn</a> for most update-to-date recruitment information.<p>
	  
			
		  </div>
	  
		  <!-- Right Column: Contact & Links -->
		  <div class="about-contact">
        <h3>Services (Selected)</h3>
        <ul class="about-list">
          <li><strong>Associate Editor</strong>: • <em>Proceedings of the National Academy of Sciences (ad-hoc)</em> • <em>Scientific Report</em> • <em>IEEE Transactions on Medical Imaging</em> • <em>IEEE Transactions on Artificial Intelligence</em> • <em>Data Intelligence</em>, etc.</li>
          <li><strong>Committee Service</strong>: • <em>Founding Chair of MICCAI on Multiscale Multimodal Medical Imaging</em> • <em>Founding Chair of NeurIPS Workshop on Advancements in Medical Foundation Models</em> • <em>Founding Chair of SAGES Critical View of Safety Challenge</em> • <em>Founding Chair of AAAI Workshop: AI for Health: Leveraging Artificial Intelligence to Revolutionize Healthcare</em> etc.<br />Area Chair of the • <em>MICCAI Conference</em> • <em>IEEE Symposium on Computer-Based Medical Systems</em></li>
        </ul>
      
        <p><strong>Publications</strong>: 150+ peer-reviewed publications, h-index 47. See my <a target="_blank" href="https://scholar.google.com/citations?user=MjkwwiQAAAAJ&hl=en">Google Scholar</a> for more information.</p>
        
		  </div>
		</div>
	  </section>
	  

    <hr/>

    <!-- RECENT UPDATES -->
    <section id="updates" class="updates">
      <h2>Recent Updates</h2>
	  <div id="updates-content" class="collapsible collapsed">
      <div class="update">
        <div class="update-date">February 6th, 2025</div>
        <p>
        Our review paper, "Artificial General Intelligence for Medical Imaging Analysis", is selected as the Feature Article for <em>IEEE Reviews in Biomedical Engineering</em>. 
        <img src="./img/TRBME_v1.png" width="600"/>
        </p>
        </div>
        <div class="update">
        <div class="update-date">February 5th, 2025</div>
        <p>
        Our paper, "MediViSTA: Medical Video Segmentation via Temporal Fusion SAM Adaptation for Echocardiography", has been accepted by the <em>IEEE Journal of Biomedical and Health Informatics</em>. Read the full paper <a target="_blank" href="https://ieeexplore.ieee.org/document/10878483">—>here</a>. <br />
        This work introduces a 2D SAM adaptation model for medical video segmentation tailored for echocardiography. By incorporating parameter-efficient learning techniques (LoRA) and temporal fusion strategies, our approach effectively optimizes SAM, enhancing segmentation accuracy and ensuring consistency across frames.
        </p>
        </div>

        <div class="update">
          <div class="update-date">Janurary 22nd, 2025</div>
          <p>
          Two papers accpeted by <em>ICLR</em> 2025: <br />
          “Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data”, our collaborative work with Prof. Ninghao Liu at UGA, tackles the challenge of making large multimodal models not only more accurate in domain-specific tasks but also capable of delivering clear, interpretable explanations. Read the full paper <a target="_blank" href="https://openreview.net/forum?id=lHbLpwbEyt">—>here</a>. <br />
          "ECHOPulse: ECG Controlled Echocardio-gram Video Generation", leverages single-channel ECG signals (like from an Apple Watch) to generate corresponding cardiac ultrasound (echocardiogram) videos, pushing the boundaries of extreme cross-modality generation. Read the full paper <a target="_blank" href="https://openreview.net/forum?id=i2r7LDjba3">—>here</a>.
          </p>
          </div>

        <div class="update">
          <div class="update-date">December 19th, 2024</div>
              <p>Our CME Review paper, "Large Language Models and Large Multimodal Models in Medical Imaging: A Primer for Physicians", has been accepted by the <em>Journal of Nuclear Medicine</em>. Read the full paper <a target="_blank" href="https://pubmed.ncbi.nlm.nih.gov/39819692/">—>here</a>.<br />
              </p>
        </div>		

        <div class="update">
          <div class="update-date">December 23rd, 2024</div>
              <p>The book "Discovering the Frontiers of Human-Robot Interaction" has been published by Springer Nature. We contributed the chapter "Robot Control via Natural Instructions Empowered by Large Language Models", which can be found <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-66656-8_19">—>here</a><br />
                We are collaborating with Harvard SEAS on research into medical robotics, with a UR5e robotic arm for our research into intelligent imaging. One of the most critical challenges in the field of control systems today is how to effectively and stably integrate Artificial General Intelligence (AGI) into robot control. In addition to our work on LLM-driven robot control, we are also conducting research on generative AI for guiding robot learning, as well as foundational models for surgical robotics.
              </p>
        </div>		

        <div class="update">
          <div class="update-date">November 14th, 2024</div>
              <p>Our patent, "Automated Detection and Management of Valvular Heart Disease Using Machine Learning" (US-20240379239-A1), is now public. Read the full document <a target="_blank" href="https://patents.google.com/patent/US20240379239A1/en">—>here</a>. 
              </p>
        </div>		


        <div class="update">
          <div class="update-date">November 13th, 2024</div>
              <p>Our long-paper submission, "MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical Question Answering", received the <strong>Distinguished Paper Award</strong> at <em>AMIA</em> 2024. Its arXiv link can be found <a target="_blank" href="https://arxiv.org/abs/2309.16035">—>here</a>. In this collaborative work with Prof. Ninghao Liu from the University of Georgia, we developed a comprehensive retrieval strategy to extract medical facts from an external knowledge base, and then incorporated them into the query prompt for the LLM. This work demonstrates the potential of model editing to enhance LLM performance, offering a practical approach to mitigate the challenges of black-box LLMs.
                <img src="./img/1731687404774.jpg" width="600"/>
              </p>
        </div>		

        <div class="update">
          <div class="update-date">November 12th, 2024</div>
              <p>
              Two podium abstracts presented at <em>AMIA</em> 2024:

                "Cross-Modal Retrieval for Alzheimer's Disease Diagnosis Using CLIP-Enhanced Dual Deep Hashing," which leverages CLIP alignment for improved cross-modal (any two modalities) retrieval.<br />
                
                and <br />
                "Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction," which uses diffusion models to predict the trajectory of cortical thickness change over time.<br />
              </p>
              
        </div>

      <div class="update">
        <div class="update-date">October 16th, 2024</div>
        <p>
        Serving as the PAR panel for NIH Academic-Industrial Partnerships for Translation of Technologies.
        </p>
        </div>

        <div class="update">
          <div class="update-date">September 25th, 2024</div>
          <p>
          Two papers accepted by <em>NeurIPS</em> 2024:<br />
          "Eye-gaze Guided Multi-modal Alignment for Medical Representation Learning", where we explored how eye-gaze tracking enhances multi-modal learning for radiology applications. Read the full paper <a target="_blank" href="https://openreview.net/forum?id=0bINeW40u4">—>here</a>. <br />
          and<br />
          "Biomedical Visual Instruction Tuning with Clinician Preference Alignment", investigated the alignment between  biomedical visual models with clinician expertise. Read the full paper <a target="_blank" href="https://openreview.net/forum?id=Eogs84mv7N#discussion">—>here</a>. 
          </p>
          </div>

        <div class="update">
          <div class="update-date">August 7th, 2024</div>
          <p>
          Our paper, "A generalist vision–language foundation model for diverse biomedical tasks", is accepted by <em>Nature Medicine</em>. The BiomedGPT model is a lightweight, open-source generalist medical AI that excels across 25 datasets and 9 tasks, achieving 16 state-of-the-art results in a diversified array of medical tasks. Read the full paper <a target="_blank" href="https://www.nature.com/articles/s41591-024-03185-2">—>here</a>.<br />
          </p>
          </div>

          <div class="update">
            <div class="update-date">July 22nd, 2024</div>
                <p>
                  <a target="_blank" href="https://aim-fm-24.github.io/NeurIPS/">AIM-FM: Advancements In Medical Foundation Models: Explainability, Robustness, Security, and Beyond</a><br />
                  We will host the First <em>NeurIPS</em> workshop AIM-FM: Advancements In Medical Foundation Models: Explainability, Robustness, Security, and Beyond. This workshop aims to explore the potential of Medical Foundation Models (MFMs) in smart medical assistance, thereby improving patient outcomes and streamlining clinical workflows, with an emphasize on the explainability, robustness, and security of the large-scale multimodal medical assistant, pushing forward its reliability and trustworthiness.
                </p>
          </div>

          <div class="update">
            <div class="update-date">Jun 27th, 2024</div>
                <p>Our Editor's Blog article, "Can AI solve the clinical data problem?", is now online at <em>Science</em>, which illuminates both the potential benefits and the challenges of integrating AI into clinical workflows. Read the full article <a target="_blank" href="https://www.science.org/content/blog-post/can-ai-solve-clinical-data-problem">—>here</a>.                   
                </p>
          </div>

          <div class="update">
            <div class="update-date">May 5th, 2024</div>
                <p>Call for Papers: <em>IEEE Transactions on Neural Networks and Learning Systems</em> Special Issue: <strong>Advancements in Foundation Models</strong><br />
                  Guest Editors:<br />
                  Tianming Liu, University of Georgia, USA<br />
                  Xiang Li, Massachusetts General Hospital and Harvard Medical School, USA<br />
                  Hao Chen, Hong Kong University of Science and Technology, Hong Kong, China<br />
                  Yixuan Yuan, Chinese University of Hong Kong, Hong Kong, China<br />
                  Anirban Mukhopadhyay, TU Darmstadt, Germany  <br />
                  Learn more at <a target="_blank" href="https://cis.ieee.org/images/files/Publications/TNNLS/special-issues/TNNLS_SI_CFP_Advancements_in_Foundation_Models.pdf">—>here</a>.                   
                </p>
          </div>

          <div class="update">
            <div class="update-date">May 1st, 2024</div>
                <p>Our Position Paper, "TrustLLM: Trustworthiness in Large Language Models", has been accepted by <em>ICML</em> 2024. This work was also recognized by Hugging Face as its daily paper and ranked #1 in likes. Read the full article <a target="_blank" href="https://openreview.net/forum?id=bWUU0LwwMp">—>here</a>
                </p>
          </div>

          <div class="update">
            <div class="update-date">April 26th, 2024</div>
                <p><a target="_blank" href="https://mmmi2024.github.io/">MMMI 2024</a><br />
                  This year, we will continue organizing the MICCAI workshop on Multiscale Multimodal Medical Imaging (MMMI 2024), together with the wonderful team from FAU and TUM organizing the 1st Workshop on Machine Learning for Multimodal/-sensor Healthcare Data (ML4MHD 2024).
                <img src="./img/MMMI2024.jpg" width="600"/>
                </p>
          </div>

          <div class="update">
            <div class="update-date">April 14th, 2024</div>
                <p>Five papers accepted by <em>MICCAI</em> 2024:<br />
                "Volumetric Conditional Score-based Residual Diffusion Model for PET Denoising" <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-72104-5_72">—></a><br />
                "F2TNet: FMRI to T1w MRI Knowledge Transfer Network for Brain Multi-phenotype Prediction" <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-72120-5_25">—></a><br />
                "Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction" <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-72069-7_8">—></a><br />
                "Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompt" <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-72390-2_23">—></a><br />
                "Hallucination Index: An Image Quality Metric for Generative Reconstruction Models" <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-72117-5_42">—></a><br />
                </p>
          </div>
          

      <div class="update">
		<div class="update-date">December 30th, 2023</div>
		<p>Started serving as the Associate Editor for <em>Data Intelligence</em>.
		</p></div>				
		
		<div class="update">
			<div class="update-date">December 18th, 2023</div>
					<p>Started serving as the Associate Editor for <em>IEEE Transactions on Artificial Intelligence</em>.
					</p>
		</div>			
    <div class="update">
					<div class="update-date">December 13th, 2023</div>
					<p>Our work on the development of a cyclic image to/from text model, "AdaMatch-Cyclic", has been accepted by <em>ACL</em> 2024 and is now availabl <a target="_blank" href="https://aclanthology.org/2024.acl-long.514/">—>here</a>. In this work we explored the fine-grained mapping between chest x-ray images and their corresponding radiology reports and developed generative model for the image and text correspondingly. Preliminary result shows that the cyclic generation model can accurately capture word-patch relationsihp and perform effective text generation (report writing) and image generation. 
					</p>
		</div>

		
		<div class="update">
			<div class="update-date">November 3rd, 2023</div>
					<p>Started serving as the Associate Editor for <em>BMC Biomedical Engineering</em>.
					</p>
		</div>	
					
		<div class="update">
			<div class="update-date">October 29th, 2023</div>
					<p>Our work on evaluating the medical image understanding capability of GPT-4v, "Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V", is now availabl on <a target="_blank" href="https://arxiv.org/abs/2310.19061">arXiv</a>. In this work, we found that the current version of GPT-4V is not recommended for real-world diagnostics due to its unreliable and suboptimal accuracy in responding to diagnostic medical questions. In addition, we delineate seven unique facets of GPT-4V's behavior in medical VQA task, highlighting its constraints within this complex arena. The paper has been recommended by Hugging Face <a target="_blank" href="https://huggingface.co/papers?date=2023-10-31">—>here</a>.
					</p>
		</div>	
			
		<div class="update">
			<div class="update-date">October 20th, 2023</div>
					<p>Our work on graph transformer representation of dynamic brain imaging data, "Large-scale Graph Representation Learning of Dynamic Brain Connectome with Transformers", was accepted by <em>NeurIPS 2023 Temporal Graph Learning Workshop</em>. In this collaborative work with Dr. Byung-Hoon Kim from Yonsei University College of Medicine, we developed a representation learning framework for modeling dynamic functional connectivity with graph transformers. By the novel "connectome embedding" concept developed in this work, we are able to characterize the position, structure, and time information of the functional connectivity graph within an integrated embedding. Experiment results from multiple fMRI datasets show state-of-the-art performance of the proposed framework in gender classification and age regression tasks. Its OpenReview link can be found <a target="_blank" href="https://openreview.net/forum?id=WvFHDkIhmp">—>here</a>.
					</p>
		</div>	
			
		<div class="update">
			<div class="update-date">September 24th, 2023</div>
					<p>Our recent paper, "MediViSTA-SAM: Zero-shot Medical Video Analysis with Spatio-temporal SAM Adaptation", is now on <a target="_blank" href="https://arxiv.org/abs/2309.13539">arXiv</a>. As the Segmentation Anything Model (SAM) has become prominent for its generalization abilities, we developed MediViSTA-SAM for adapting SAM to medical video data. The model can effectively capture both long and short-range temporal dependency structures through its cross-frame attention mechanism. Furthermore, MediViSTA-SAM utilizes a novel U-shaped encoder and an adapted mask decoder to handle objects of various dimensions. MediViSTA-SAM was tested on echocardiography datasets from multiple vendors/institutions and achieved state-of-the-art performance in segmenting the left ventricle and left atrium from echocardiogram video data.
					</p>
		</div>			
			
		<div class="update">
			<div class="update-date">September 22nd, 2023</div>
					<p>Our research on fine-tuning LLM for radiation oncology, "RadOnc-GPT: A Large Language Model for Radiation Oncology", is now on <a target="_blank" href="https://arxiv.org/abs/2309.10160">arXiv</a>. RadOnc-GPT is fined-tuned using an extensive dataset encompassing radiation oncology patient records and clinical observations sourced from Mayo Clinic Arizona. Clinical implication of RadOnc-GPT is investigated on three tasks: generating radiotherapy treatment plans, determining optimal radiation modalities, and providing diagnostic descriptions/ICD codes based on patient diagnostic details. When benchmarked against general-domain large language models, RadOnc-GPT has superior performance, characterized by its enhanced clarity, specificity, and clinical relevance. This work underscores the revolutionary potential of domain-centric language models for healthcare practice that demands specialized expertise.
					</p>
		</div>			
			
		<div class="update">
			<div class="update-date">September 22nd, 2023</div>
					<p>Started working as the Affiliate Faculty Member at the <a target="_blank" href="https://www.harvard.edu/kempner-institute/">Kempner Institute for Natural and Artificial Intelligence</a> of Harvard University.
					</p>
		</div>			
			
		<div class="update">
			<div class="update-date">September 16th, 2023</div>
					<p>Our recent paper, "MA-SAM: Modality-agnostic SAM Adaptation for 3D Medical Image Segmentation", is now on <a target="_blank" href="https://arxiv.org/abs/2309.08842">arXiv</a>. While the Segment Anything Model (SAM) has showcased stellar results in general-domain image segmentation, it needs further adaptation to be working on medical images due to its limitation to 2-D images. Recognizing the importance of the third dimension, either volumetric or temporal, we developed MA-SAM, a modality-agnostic framework tailored for a wide variety of 3D medical imaging modalities, equipping our efficient parameter fine-tuning strategy and the 3D adapters design. We tested MA-SAM on ten diverse datasets spanning 3D CT, 3D MRI, and surgical videos. The model can achieve superior performance over major medical segmentation algorithms, including nnU-Net, by notable margins. The code of MA-SAM is accessible on <a target="_blank" href="https://github.com/cchen-cc/MA-SAM">GitHub</a>.
					</p>
		</div>	
		
		<div class="update">
			<div class="update-date">September 10th, 2023</div>
					<p>Our paper on analyzing ChatGPT-generated language data and its comparison to human-written language, "Differentiate ChatGPT-generated and Human-written Medical Texts", has been accepted by the <a target="_blank" href="mededu.jmir.org">JMIR Medical Education (JME)</a>. We analyzed the linguistic features of the ChatGPT-generated txt versus human-generated to uncover differences in vocabulary, part-of-speech, dependency, sentiment, perplexity, etc. We also developed A BERT-based model to detect medical texts generated by ChatGPT with high accuracy. Its arXiv version can be found <a target="_blank" href="https://arxiv.org/abs/2304.11567">—>here</a>.
					</p>
		</div>
			
		<div class="update">
			<div class="update-date">August 29th, 2023</div>
					<p>Our work on fine-tuning LLaMA2 for radiology, "Radiology-Llama2: Best-in-Class Large Language Model for Radiology", is now on <a target="_blank" href="https://arxiv.org/abs/2309.08842">arXiv</a>. Radiology-Llama2 is a large language model specialized for radiology by fine-tuning a large dataset of radiology reports, targeting at generating coherent and clinically useful impressions from radiological findings. Quantitative evaluations using ROUGE metrics on the MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves state-of-the-art performance compared to other large language models. Additional assessments by radiology experts highlight the model's strengths in understandability, coherence, relevance, conciseness, and clinical utility. The work illustrates the potential of domain-specific language models designed and tuned for specialized fields in healthcare. When properly evaluated and deployed, such models can transform fields like radiology by automating tasks and providing clinical decision-making support.
					</p>
		</div>
			
		<div class="update">
			<div class="update-date">August 18th, 2023</div>
					<p>Our paper on the fine-tuning and adaptation of LLMs on radiology report data to develop foundation models for radiology, entitled "Tailoring Large Language Models to Radiology: A preliminary approach to LLM adaptation for a highly specialized domain", is accepted by <i>Machine Learning in Medical Imaging (MLMI 2023)</i> for oral presentation. In this work, we developed a domain-specific large language model (LLM) for radiology by fine-tuning LLaMA on the findings-impression pairs extracted from the radiology reports in the MIMIC dataset. The work demonstrated promising performance and show potential applications in radiological diagnosis, research, and communication, suggesting a promising direction for tailored medical language models with conversational abilities. Its arXiv version can be found <a target="_blank" href="https://arxiv.org/abs/2306.08666">—>here</a>.
					</p>
		</div>
			
		<div class="update">
			<div class="update-date">August 18th, 2023</div>
					<p>Our paper on view classification for echocardiogram ultrasound imaging, entitled "Multi-task Learning for Hierarchically-Structured Images: Study on Echocardiogram View Classification", has been accepted by the <i>MICCAI Workshop of Special Interest Group on Medical Ultrasound</i>. In this collaborative work with cardiologists and sonographers at Massachusetts General Hospital and Brigham and Women's Hospital, we developed a Multi-task Residual Neural Network (MTRNN) with a hierarchically structured output for echocardiogram view classification, showcasing superior performance. The design of this work can be extended to other data classification scenarios with hierarchical data labels as well. 
					</p>
		</div>		
					
		<div class="update">
			<div class="update-date">August 8th, 2023</div>
					<p>Our abstract on predicting the early discharge of patients after receiving Transcatheter Aortic Valve Replacement (TAVR), entitled "Machine Learning Model for the Prediction of Early Discharge of Patient Underwent Transcatheter Aortic Valve Replacement Using Electronic Medical Record", has been accepted to be presented at the <i>American Heart Association’s annual Scientific Sessions 2023 (AHA 2023)</i>. In this collaborative work with cardiologists at Massachusetts General Hospital and Brigham and Women's Hospital, we collected electronic health record (EHR) data from patients who underwent TAVR and trained a machine learning model to predict the early discharge (within two days) of patients, which has been critical for accelerating patient recovery and saving costs. 
					</p>
		</div>			
			
		<div class="update">
			<div class="update-date">July 7th, 2023</div>
					<p>Our abstract on differentiating people with Alzheimer's disease and normal aging by their speech audio and transcripts, entitled "Alzheimer's Disease Prediction through Patients Speech Transcript Using Pre-trained Language Models", has been accepted to the presented at the <i>American Medical Informatics Association (AMIA) 2023 Annual Symposium</i>. This study explores methods for detecting combineing pre-trained language models, Graph Neural Networks (GNN), text data augmentation by ChatGPT, and contrastive learning for text-audio data fusion. Its arXiv version can be found <a target="_blank" href="https://arxiv.org/abs/2307.02514">—>here</a>.
					</p>
		</div>			
					
		<div class="update">
			<div class="update-date">June 1st, 2023</div>
					<p>Joined the rank of Assistant Professor at Department of Radiology, Harvard Medical School and Massachusetts General Hospital.</p>
		</div>
			
		<div class="update">
			<div class="update-date">May 6th, 2023</div>
					<p><a target="_blank" href="https://mmmi2023.github.io/">MMMI2023</a><br />
					This year we will continue organizing the MICCAI workshop on Multiscale Multimodal Medical Imaging (MMMI 2023). If you work on related areas, we are looking forward to your paper submission. The submission deadline is <strike>July 14th</strike>July 31st, 2023.
					<img src="./img/MMMI2023.png" width="600"/>
					</p>
		</div>
			
		<div class="update">
			<div class="update-date">March 14th, 2023</div>
					<p>Our paper on the open community bench-testing platform for reserach on neuron tracing, "BigNeuron: a resource to benchmark and predict performance of algorithms for automated tracing of neurons in light microscopy datasets" is accepted by <i>Nature Methods</i> and available at <a target="_blank" href="https://www.nature.com/articles/s41592-023-01848-5">—>here</a>. This work was initiated and devloped by the team led by <a target="_blank" href="http://penglab.com/">Dr. Hanchuan Peng</a>, who's a member of my Phd. committee.
					</p>
		</div>	
			
		<div class="update">
			<div class="update-date">February 28th, 2023</div>
					<p>Our absract on predicting the clinical outcomes (length-of-stay and readmission) of aortic stenosis patients after receiving Surgical Aortic Valve Replacement (SAVR)or Transcatheter Aortic Valve Replacement (TAVR), entitled "Machine Learning Model for Aortic Stenosis Patient Outcome Prediction", has been accepted to be presented at the <i>AMIA 2023 Clinical Informatics Conference 2023</i>. Risk stratification and patient outcome prediction are helpful for physicians in guiding clinical decision-making and the hospital’s resource allocation and patient management. In this work on preoperative aortic stenosis (AS) patient outcome prediction, we showed that by applying machine learning methods to a comprehensive list of the patient electronic health record (EHR) data, superior prediction performance could be achieved compared with the current regression-based risk score systems. 
					</p>
		</div>	
			
		<div class="update">
			<div class="update-date">December 16th, 2022</div>
					<p>Our first paper on the language processing in medical domain, "ClinicalRadioBERT: Knowledge-Infused Few Shot Learning for Clinical Notes Named Entity Recognition" is accepted by International Workshop on Machine Learning in Medical Imaging (MLMI 2022)and available at <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-21014-3_28">—>here</a>. In this paper, we proposed the knowledge-infused few-shot learning (KI-FSL) approach to develop the ClinicalRadioBERT model for the task of radiotherapy clinical notes named entity recognition (NER)..
					</p>
		</div>
		
		<div class="update">
			<div class="update-date">October 13th, 2022</div>
					<p>Proceeding of MMMI 2022 as part of the MICCAI 2022 conference proceeding, published as Lecture Notes in Computer Science (LNCS) book series, is now available for free access via the MMMI 2022 official website at <a target="_blank" href="https://mmmi2022.github.io/">—>here</a>. Note that the full proceeding is only accessible directed from that website, and will be available for 4 weeks. Cover of this proceeding can be downloaded <a target="_blank" href="./pdf/2022_Bookmatter_MultiscaleMultimodalMedicalIma.pdf" id="pdf">—>here</a>.
					</p>
		</div>
							
		<div class="update">
			<div class="update-date">July 1st, 2022</div>
					<p>We have been awarded by the National Institutes of Health for 2-year support of our project, "Identification of Multi-modal Imaging Biomarkers for Early Prediction of MCI-AD Conversion via Multigraph Representation" (1R03AG078625-01). Alzheimer's disease (AD) will result in cognitive decline and dementia and is a leading cause of mortality in the growing elderly population. As a progressive disease, AD typically has an insidious onset with overlapping clinical features with the transitional state of Mild Cognitive Impairment (MCI). Analysis of the relationship between MCI and AD with a focus on the converting factors, by the co-modeling of a wide array of imaging methods, can help us in a deeper understanding of the disease mechanism, which can lead to more accurate early diagnosis and identification of better intervention techniques.
					</p>
		</div>    
					
		<div class="update">
			<div class="update-date">April 5th, 2022</div>
					<p><a target="_blank" href="https://mmmi2022.github.io/">MMMI2022</a><br />
					This year we will continue organizing the MICCAI workshop on Multiscale Multimodal Medical Imaging (MMMI 2022). If you work on related areas, we are looking forward to your paper submission. The submission deadline is July 22nd, 2022.
					<img src="./img/MMMI2022.png" width="600"/>
					</p>
		</div>	
			
		<div class="update">
			<div class="update-date">March 15th, 2022</div>
					<p>We have been awarded by the National Institutes of Health for a 4-year support of our project, "Deep Learning Based Phenotyping and Treatment Optimization of Heart Failure with Preserved Ejection Fraction" (1R01HL159183-01A1). Heart failure with preserved ejection fraction (HFpEF) is a major public health problem that is rising in prevalence with the aging population. By performing deep phenotyping of the patients from their Cardiac Magnetic Resonance images and Electronic Health Record simultaneously, we aim to provide phenotypic-specific, individualized treatment optimization based on the current massive amount of clinical data using deep learning.</p>
		</div>	
					
		<div class="update">
			<div class="update-date">February 4th, 2022</div>
					<p>I have received the 2021 MGH Thrall Innovation Grants Award for funding my research of lung cancer screening by transformed chest X-ray imaging. The project is called "Chest Radiographs-based Lung Cancer Screening by the DeepProjection Technique" based on the DeepProjection technique previously developed by us which can generate near-real volumetric 3D CT image from a single 2D Chest Radiograph (CXR) image. By replacing CT scans with CXR-generated pseudo-CTs for lung cancer screening, we can reduce radiation doses, scan time and cost for the screening, as well as better availability for remote healthcare sites. News of this award can be found <a target="_blank" href="https://gordon.mgh.harvard.edu/gc/mgh-thrall-innovation-grants-awarded-to-two-gordon-center-faculty/">—>here</a>, and covered by the Rad Times over <a target="_blank" href="https://radhub.massgeneral.org/radtimes/2022/01/27/james-thrall-innovation-grant-award-recipients/">—>here</a>.</p>
		</div>	
			
		<div class="update">
			<div class="update-date">December 1st, 2021</div>
					<p>Our paper "Artificial Intelligence and Machine Learning in Radiology: Opportunities, Challenges, Pitfalls, and Criteria for Success" is awarded the <a target="_blank" href="https://www.journals.elsevier.com/journal-of-the-american-college-of-radiology/most-cited-articles">"Most Cited Articles"</a> of <i>Journal of the American College of Radiology</i>.</p>
					<p><img src="./img/JACR.png" width="600"/></p>
		</div>
					
		<div class="update">
			<div class="update-date">September 27th, 2021</div>
					<p>We have the pleasure to host the research topic of "Multi-Dimensional Characterization of Neuropsychiatric Disorders" in <i>Frontiers in Neuroscience - Brain Imaging Methods</i>. Link to this research topic can be found <a target="_blank" href="https://www.frontiersin.org/research-topics/24953/multi-dimensional-characterization-of-neuropsychiatric-disorders">—>here</a>. All research related to the identification of multi-modal biomarkers for psychiatric disorders and multi-modal fusion methodologies are welcomed! Deadline for the intention-to-submit is <u>December 23rd, 2021</u>.</p>
		</div>
					
		<div class="update">
			<div class="update-date">August 13th, 2021</div>
					<p>Our work on multi-hospital federated learning for combined analysis of chest X-ray images and electronic health records data towards the prediction of COVID-19 patient's risk in the emergency department has been accepted by <i>Nature Medicine</i>. Its link can be found <a target="_blank" href="https://www.nature.com/articles/s41591-021-01506-3">—>here</a> (open access). This work, led by Mass General Brigham and NVIDIA, shows how federated learning enables creation of robust AI models for healthcare and other industries constrained by confidential and sparse data. Coverage of this work by NVidia can be found <a target="_blank" href="https://blogs.nvidia.com/blog/2021/09/15/federated-learning-nature-medicine/">—>here</a>.</p>
					<iframe width="560" height="315" src="https://www.youtube.com/embed/cOXVrtkv6FE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		</div>
		<div class="update">
			<div class="update-date">March 23rd, 2021</div>
					<p>Our work on severe outcome prediction and a clinical risk score (CO-RISK score) system for COVID-19 patient triage at emergency department is now online at <a target="_blank" href="https://arxiv.org/abs/2103.11269" id="arxiv">arXiv</a>.</p>
					<p>In this study we constructed the "MGB Cohort", a database covering all patients suspected of COVID-19 presented to the emergency department at the four hospital sites of the MGB system. A total of 11,060 patients were used in the model development and validation, according to our inclusion/exclusion criteria. A deep learning system based on the architecture of Deep and Cross network was developed to predict the patient's outcome in 24/72 hours based on the EHR and imaging (CXR) data up to the initial present to the emergency department. </p>
		</div>
	
	</div>
  <a id="toggle-updates" href="javascript:void(0)">See more...</a>
	



    </section>

    <hr/>

    <!-- PUBLICATIONS -->
    <section id="publications" class="publications">
      <h2>Selected publications in the last 3 years</h2>
	  <div id="publication-content" class="collapsible collapsed">
      <div class="publication">
        <p>
          Bridging Medical Imaging and Reports: Learning Radiologist's Nuances via Fine-Grained Multi-Modal Alignment<br /><a target="_blank" href="https://www.researchsquare.com/article/rs-6002276/v1">—></a>
        </p>
        <p>
          Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data<br />(<i>ICLR</i> 2025) <a target="_blank" href="https://openreview.net/forum?id=lHbLpwbEyt">—></a>
        </p>
        <p>
          ECHOPulse: ECG controlled echocardio-grams video generation<br />(<i>ICLR</i> 2025) <a target="_blank" href="https://openreview.net/forum?id=i2r7LDjba3">—></a>
        </p>        
        <p>
          MediViSTA: Medical Video Segmentation via Temporal Fusion SAM Adaptation for Echocardiography<br />(<i>IEEE JBHI</i> 2025) <a target="_blank" href="https://ieeexplore.ieee.org/document/10878483">—></a>
        </p> 
        <p>
          Distribution-aware Fairness Learning in Medical Image Segmentation From A Control-Theoretic Perspective<br /><a target="_blank" href="https://arxiv.org/abs/2502.00619">—></a>
        </p>
        <p>
          Large Language Models and Large Multimodal Models in Medical Imaging: A Primer for Physicians<br />(<i>JNM</i> 2025 CME Article) <a target="_blank" href="https://jnm.snmjournals.org/content/66/2/173.abstract">—></a>
        </p>
        <p>
          Auggpt: Leveraging chatgpt for text data augmentation<br />(<i>IEEE Big Data</i> 2025) <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10858342">—></a>
        </p>        
        <p>
          Biomedical visual instruction tuning with clinician preference alignment<br />(<i>NeurIPS</i> 2024) <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/aec33ab89b5986605cd7c331396e7e5c-Abstract-Datasets_and_Benchmarks_Track.html">—></a>
        </p>
        <p>
          Eye-gaze guided multi-modal alignment for medical representation learning<br />(<i>NeurIPS</i> 2024) <a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/0b9536e186a77feff516893a5f393f7a-Abstract-Conference.html">—></a>
        </p>
        <p>
          Artificial General Intelligence for Medical Imaging Analysis<br />(<i>IEEE Reviews in Biomedical Engineering</i> 2024) <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10746601">—></a>
        </p>
        <p>
          Conditional Score-Based Diffusion Model for Cortical Thickness Trajectory Prediction<br />(<i>MICCAI</i> 2024) <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-72069-7_8">—></a>
        </p>
        <p>
          Medical Image Synthesis via Fine-Grained Image-Text Alignment and Anatomy-Pathology Prompting<br />(<i>MICCAI</i> 2024) <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-72390-2_23">—></a>
        </p>
        <p>
          A generalist vision–language foundation model for diverse biomedical tasks<br />(<i>Nature Medicine</i> 2024) <a target="_blank" href="https://www.nature.com/articles/s41591-024-03185-2">—></a>
        </p>
        <p>
          Fine-Grained Image-Text Alignment in Medical Imaging Enables Explainable Cyclic Image-Report Generation<br />(<i>ACL</i> 2024) <a target="_blank" href="https://aclanthology.org/2024.acl-long.514/">—></a>
        </p>
        <p>
          Zero-Shot Novel View Synthesis of Wrist X-Rays Using Latent Diffusion Model<br />(<i>ISBI</i> 2024) <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10635244">—></a>
        </p>
        <p>
          High-Resolution 3d Ct Synthesis From Bidirectional X-Ray Images Using 3d Diffusion Model<br />(<i>ISBI</i> 2024) <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10635648">—></a>
        </p>
        <p>
          An iterative optimizing framework for radiology report summarization with ChatGPT<br />(<i>IEEE TAI</i> 2024) <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10433180">—></a>
        </p>
        <p>
          MKRAG: Medical Knowledge Retrieval Augmented Generation for Medical Question Answering<br />(<i>AMIA</i> 2024 Best Paper) <a target="_blank" href="https://github.com/sycny/MKRAG">—></a>
        </p>
        <p>
          Zero-shot relation triplet extraction as Next-Sentence Prediction<br />(<i>Knowledge-Based Systems
          </i> 2024) <a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0950705124011419">—></a>
        </p> 
        <p>
          Motion Correction and Super-Resolution for Multi-slice Cardiac Magnetic Resonance Imaging via an End-to-End Deep Learning Approach<br />(<i>Computerized Medical Imaging and Graphics</i> 2024) <a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0895611124000661">—></a>
        </p>         
        <p>
          Mask-guided BERT for few-shot text classification<br />(<i>Neurocomputing</i> 2024) <a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S092523122401347X">—></a>
        </p>                                                                          
        <p>
          Coarse-to-fine Knowledge Graph Domain Adaptation based on Distantly-supervised Iterative Training<br />(<i>BIBM</i> 2023) <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/10385649">—></a>
        </p>
        <p>
          Graph-Based Counterfactual Causal Inference Modeling for Neuroimaging Analysis<br />(<i>MICCAI Workshop</i> 2023) <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-47425-5_19">—></a>
        </p>
        <p>
          Tailoring Large Language Models to Radiology: A Preliminary Approach to LLM Adaptation for a Highly Specialized Domain<br />(<i>MICCAI Workshop</i> 2023) <a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-031-45673-2_46">—></a>
        </p>        
      </div>
      
	  
	</div>
  <a id="toggle-publication" href="javascript:void(0)">See more...</a>
      <!-- More publications as needed -->
    </section>

    <!-- FOOTER -->
    <footer id="contact">
      <h3>Contact me at 399 Revolution Dr. 11th Floor, Somervile, MA 02145,<br />Email: xiangli.shaun[at]gmail.com</h3>
      <p>
        <a target="_blank" href="https://researchers.mgh.harvard.edu/profile?profile_id=15451263">MGH Researcher</a> |
        <a target="_blank" href="https://dblp.org/pid/40/1491-1.html">DBLP</a> |
        <a target="_blank" href="https://www.scopus.com/authid/detail.uri?authorId=27168120000">Scopus</a> |
        <a target="_blank" href="https://orcid.org/0000-0002-9851-6376">ORCID</a> |
        <a target="_blank" href="https://camca.mgh.harvard.edu/">CAMCA</a> |
        <a target="_blank" href="https://mastodon.social/@xiangli_shaun">Mastodon</a>
      </p>
      <p>
        Last updated: April 2nd, 2025
      </p>
    </footer>
  </div>
  <script>
	document.getElementById('toggle-updates').addEventListener('click', function () {
	  const content = document.getElementById('updates-content');
	  const isCollapsed = content.classList.contains('collapsed');
	  
	  content.classList.toggle('collapsed');
	  content.classList.toggle('expanded');
	  this.textContent = isCollapsed ? 'See less...' : 'See more...';
	});
	document.getElementById('toggle-publication').addEventListener('click', function () {
	  const content = document.getElementById('publication-content');
	  const isCollapsed = content.classList.contains('collapsed');
	  
	  content.classList.toggle('collapsed');
	  content.classList.toggle('expanded');
	  this.textContent = isCollapsed ? 'See less...' : 'See more...';
	});
  </script>
</body>
</html>
