<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-GB">
<head>
	<title>Homepage of Xiang Li</title>
	<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
	<meta name="description" content="Homepage of Xiang Li" />
	<meta name="keywords" content="Xiang Li" />
	<meta name="robots" content="index, follow" />
	<link rel="stylesheet" type="text/css" href="screen.css" media="screen" />
<script>
 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-40545637-1', 'uga.edu');
 ga('send', 'pageview');

</script>
</head>
<body>

<div id="header">
	<h3>Xiang "Shaun" Li, Research Fellow, Clinical Data Science Center, Harvard Medical School / Massachusetts General Hospital</h3>
	<ul>
		<li> a </li>
		<li><a href="index.html">About Me</a></li>
		<li><a href="publication.html">Publications</a></li>
		<li><a href="projects.html">Projects</a></li>
		
	</ul>
	<p id="layoutdims">......</p>
</div>
<div class="colmask leftmenu">
	<div class="colleft">
		<div class="col1">
		<h2 id="SCD">Semantic Segmentation Solution for SCD Diagnosis:</h2>
		<h4 align="justify">Red blood cell (RBC) segmentation and classification from biomedical images is a crucial step for the diagnosis of sickle cell disease (SCD). </h4>
		<p align="center"><img src="./img/project_RBC01.png" /></p>
		<figcaption align="center">Four types of RBCs. Cells of classes 2-4 have more variations in shape and texture, which usually marks a deficit in their functionalities.</figcaption>
		<h4 align="justify">Computer-aided programs that can perform automatic segmentation and classification of RBC images will greatly alleviate labor work of physicians and expedite diagnosis process. A collaborative work between <a href="https://www.ccds.io/">MGH/BWH Center for Clinical Data Science</a> and <a href="http://www.bibdr.org/en/">Center for Data Science at Peking University</a>, we are developing and deploying a semantic segmentation framework to simultaneously detect and classify RBCs from raw microscopic images, called deformable U-Net (dU-net).</h4>
		<p align="center"><img src="./img/project_RBC02.png" /></p>
		<figcaption align="center">Architecture of the dU-Net developed in this project. Notice that deformable convolution layers are used to replace normal convolution in a classical U-Net.</figcaption>
		<h4 align="justify">Testing on preliminary data consisting of 314 images from 5 different SCD patients shows that dU-net framework achieves best segmentation/classification accuracy within an integrated workflow, outperforming both traditional unsupervised methods and classical U-Net structure. The extra deformable operation used in dU-Net makes the prediction model more robust towards variations in the size, shape and viewpoint of the cells (check the deformed sampling locations illustrated in figure below).</h4>
		<Table><tr align="center"><td><img src="./img/project_RBC03.png" height="250"/></td><td><img src="./img/project_RBC04.png" height="250"/></td></tr>
		<tr><td><figcaption align="center">Binary segmentation results by different methods: (a) raw image, (b) Ilastik, (c) region growing, (d) U-Net, (e) dU-Net, (f) ground truth. .</figcaption></td>
		<td><figcaption align="center">Semantic segmentation results by U-Net and dU-Net. Different colors indicate different RBC types. Deformed sampling locations centered at the red points are shown to the right.</figcaption></td></tr>
		</table>
		<h3 align="justify" id="SCD_paper">Related publication:</h3>
		<h4 align="justify">A summary of dU-Net and the preliminary results has been accepted by MICCAI 2018, which is also at <a href="https://arxiv.org/abs/1710.08149" id="arxiv">arXiv</a>.</h4>
		<h3 align="justify" id="SCD_code">Source code:</h3>
		<h4 align="justify">Source code of 2D/3D dU-Net can be found at GitHub links: <a href="https://github.com/XiangLi-Shaun/deformableConvolution_3D">3D deformable convolution network</a> and <a href="https://github.com/XiangLi-Shaun/deformableConvolution_2D">2D deformable convolution network.</a></h4>
		<br /><hr><br />
		<h2 id="PMX">Pneumothorax prescreening on CT images:</h2>
		<h4 align="justify">Pneumothorax is a potentially life threatening condition characterized by the abnormal presence of free air in the thorax. Tension pneumothorax is widely considered an emergent critical finding, that requires special communication of the imaging results. For the detection of pneumothorax, timeliness can be challenging when chest CT examinations cannot be reviewed by the radiologists immediately. To address this chanllege, at <a href="https://www.massgeneral.org/Imaging/">MGH Radiology</a> and <a href="https://www.ccds.io/">MGH/BWH Center for Clinical Data Science</a> we are building a deep learning-enabled system to autmatically detect the presence of pneumothorax on chest CT imagesupon completion of scanning. The system can alert the radiologists and other attending providers to reprioritize the CT work list for cases likely to harbor a pneumothorax. </h4>
		<p align="center"><img src="./img/project_PMX01.png"  height="250"/></p>
		<figcaption align="center">Illustration of the need for a rapid screening system to re-prioritizing the work list for hospital management.</figcaption>
		<h4 align="justify">The pilot system has been tested in a pseudo-online manner with perfect sensitivity, illustrated in the figures below. The system will be further integrated into the clinical workflow at the point of completion of each chest CT scan, making it useful for patients with pre-existing chest diseases that place them at higher risk for pneumothorax, and for patients presenting in the emergency department for rapid triage. </h4>
		<p align="center"><img src="./img/project_PMX02.png"  height="250"/></p>
		<figcaption align="center">Visualization of pneumothorax detection results (all positive) from nine patients, shown in axial view.</figcaption>
		<h3 align="justify" id="PMX_news">Media coverage:</h3>
		<h4>Presentation of the system in ARRS 2018 is covered by auntminnie, the report can be found <a href="https://www.auntminnie.com/index.aspx?sec=sup&sub=aic&pag=dis&ItemID=120691&wf=7612">here</a>.</h4>
		<h4>This project is among the four finalist of the 2018 NVIDIA Global Impact Award, detailed information can be found <a href="https://blogs.nvidia.com/blog/2018/02/26/ai-radiology-machine-learning-global-impact-awards/">here.</a></h4>		
		<br /><hr><br />
		<h2 id="MM">Multi-modal medical image analysis:</h2>
		<h4 align="justify">Multi-modality medical imaging techniques have been increasingly applied in clinical practice and research studies, where it has been well-recognized that multi-modal imaging can reveal rich information of the imaging target. In this study at <a href="https://www.massgeneral.org/Imaging/">MGH Radiology</a> and <a href="https://www.ccds.io/">MGH/BWH Center for Clinical Data Science</a>, we are exploring how it can help advancing the accuracy and robustness for computer aided detection in a systemtic framework.</h4>
		<p align="center"><img src="./img/project_MM01.png"  height="250"/></p>
		<figcaption align="center">Multi-modal images on the same target location. (a): PET; (b): CT; (c): T1; (d): T2.</figcaption>
		 <h4 align="justify">We first proposed an algorithmic architecture for supervised multi-modal medical analysis strategies: the feature learning level fusion, classifier level fusion, and decision-making level fusion. Then based on this algorithmic architecture, we built multi-modal Convolutional Neural Networks (CNN) that performs fusion across CT, MR and PET images at various stages. </h4>
		 <p align="center"><img src="./img/project_MM02.png"  height="350"/></p>
		<figcaption align="center">Illustration of the structure for networks with different fusion level. The yellow arrows indicate the fusion location.</figcaption>
		 <h4 align="justify">For the task of detecting and segmenting Soft Tissue Sarcoma, multi-modal deep learning system shows much superior performance than single-modal systems, even using images of lowered quality. The capability of maintaining high segmentation accuracy on low-dose images with added modality of the proposed system provides a new perspective in medical image acquisition and analysis.</h4>
		<p align="center"><img src="./img/project_MM03.png"  height="250"/></p>
		<figcaption align="center">(a) Ground truth shown as yellow contour line overlaid on the T2 image. (b) Result from fusion network based on PET+CT+T1. (c) Result from single-modality network based on T2. (d-f) Results from single-modality network based on PET, CT and T1, respectively.</figcaption>
		<h3 align="justify" id="MM_paper">Related publication:</h3>
		<h4 align="justify">Our paper on the study of multimodal fusion schemes has been accepted by ISBI 2018, which is also at <a href="https://arxiv.org/abs/1711.00049" id="arxiv">arXiv</a>.</h4>
		
		<h2 id="spCNN">Self-paced learning for convolutional neural network (spCNN):</h2>
		<h4 align="justify">The development of a robust and reliable deep learning model for computer-aided diagnosis (CAD) is highly challenging due to the combination of the high heterogeneity in the medical images and the relative lack of training samples. Annotation and labeling of the medical images is much more expensive and time-consuming than other applications (e.g. natrual images) and often involves manual labor from multiple domain experts.  In this collaborative work between <a href="https://www.ccds.io/">MGH/BWH Center for Clinical Data Science</a> and Prof. Jieping Ye's group at <a href="https://medicine.umich.edu/dept/computational-medicine-bioinformatics/">Department of Computational Medicine and Bioinformatic at University of Michigan</a>, we developed multi-stage, self-paced learning framework utilizing a convolutional neural network (CNN). The key contribution of this approach is that we augment the size of training samples by refining the unlabeled instances with a self-paced learning framework. Experimental result shows that the self-pace boosted network consistently makes good prediciton with very scarce manual labels. The performance gain indicates that applications with limited training samples such as medical image analysis can benefit from self-paced learning.
		<p align="center"><img src="./img/project_spCNN02.png"  height="250"/></p>
		<figcaption align="center">Comparison of the classification accuracies among the raw CNN (last row) and spCNN under different significant levels (I for <i>p</i>=0.1, II for <i>p</i>=0.05 and III for <i>p</i>=0.025) for FDR-controlled statistical testing.</figcaption>
		<h3 align="justify" id="spCNN_paper">Related publication:</h3>
		<h4 align="justify">A summary of the methodology and preliminary resutls from applying spCNN on chest Computed Tomography (CT) images has been accepted by MLMI 2018, which is also at <a href="https://arxiv.org/abs/1707.06145" id="arxiv">arXiv</a>.</h4>
		</div>
		<div class="col2">
		<h3><a href="#SCD">RBC Semantic Segmentation</a></h3>
		<ul>
			<li><h4><a href="#SCD_paper">Related publication</a></h4></li>
			<li><h4><a href="#SCD_code">Source code</a></h4></li>
		</ul>
		<h3><a href="#PMX">Pneumothorax Prescreening</a></h3>
		<ul>
			<li><h4><a href="#PMX_news">Media coverage</a></h4></li>
		</ul>
		<h3><a href="#MM">Multi-modal Imaging</a></h3>
		<ul>
			<li><h4><a href="#MM_paper">Related publication</a></h4></li>
		</ul>
		<h3><a href="#spCNN">Self-paced Learning</a></h3>
		<ul>
			<li><h4><a href="#spCNN_paper">Related publication</a></h4></li>
		</ul>		
		</div>
	</div>
</div>
<div id="footer">
Last updated: June 1st, 2018
</div>

</body>
</html>
